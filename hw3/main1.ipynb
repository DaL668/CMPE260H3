{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f0e4871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "from a3_gym_env.envs.pendulum import CustomPendulumEnv\n",
    "import numpy as np\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "from torch.distributions import Normal\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from Modules import PolicyNetwork, ExperienceReplayBuffer, CriticNetwork\n",
    "\n",
    "# [DONE] Task 1: Start by implementing an environment interaction loop. You may refer to homework 1 for inspiration.\n",
    "# [ ] Task 2: Create and test an experience replay buffer with a random policy, which is the Gaussian distribution with arbitrary (randomly initialized) weights of the policy feed-forward network,receiving state, s, and returning the mean, mu(s) and the log_std, log_stg(s) (natural logarithm of the standard deviation) of actions.  As mentioned above, you can use a state-independent standard variance.\n",
    "# [ ] Task 3: Make an episode reward processing function to turn one-step rewards into discounted rewards-to-go: R(s_1) = sum_{t=1} gamma^{t-1} r_t, which is the discounted reward, starting from the state, s_1.\n",
    "# [ ] Task 4: Start the model by implementing a vanilla policy gradient agent, where the gradient ascent stepsare done with the average of the gradient of log-likelihood over a trajectory weight by rewards-to-go   from each state. Try different step sizes in the gradient ascent.\n",
    "# [ ] Task 5: Pendulum is a continuous action space environment. Check out the example in `Modules.py` for torch implementation of the Gaussian module.  (if you work in Julia, speak with me regarding the pendulum dynamics in Julia, and Flux for DNNs.)\n",
    "# [ ] Task 6: Add a feed-forward network for the critic, accepting the state, s=[sin(angle), cos(angle), angular velocity], and returning a scalar for the value of the state, s.\n",
    "# [ ] Task 7: Implement the generalized advantage, see Eq11-12 in the PPO paper, to be used instead of rewards-to-go.\n",
    "# [ ] Task 8: Implement the surrogate objective for the policy gradient, see Eq7, without and without clipping.\n",
    "# [ ] Task 9: Implement the total loss, see Eq9 in the PPO.\n",
    "# [ ] Task 10: Combine all together to Algorithm 1 in the PPO paper. (In your basic implementation, you can collect data with a single actor, N=1)\n",
    "# [ ] Task 11: You should see progress with default hyperparameters, but you can try tuning those to see how it will improve your results.\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "def interaction_loop():\n",
    "    env = gym.make(\"Pendulum-v1-custom\")\n",
    "    # sample hyperparameters\n",
    "    batch_size = 10000\n",
    "    epochs = 30\n",
    "    learning_rate = 1e-2\n",
    "    hidden_size = 8\n",
    "    n_layers = 2\n",
    "\n",
    "    # optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "\n",
    "    max_step = 1000\n",
    "    obs = env.reset()\n",
    "    for _ in range(max_step):\n",
    "        # get a random action in this environment\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        # render already plots the graph for you, no need to use plt\n",
    "        img = env.render()\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "    env.close()\n",
    "\n",
    "    \n",
    "# task 2 * Create and test an experience replay buffer with a random policy, which is the \n",
    "#Gaussian distribution with arbitrary (randomly initialized) weights of the policy feed-forward network,\n",
    "#receiving state, s, and returning the mean, mu(s) and the log_std, log_stg(s) \n",
    "#(natural logarithm of the standard deviation) of actions.  As mentioned above, you can use \n",
    "#a state-independent standard variance.\n",
    "def test_experience_relay_buffer():\n",
    "    env = gym.make(\"Pendulum-v1-custom\")\n",
    "    # sample hyperparameters\n",
    "    batch_size = 1000\n",
    "    epochs = 30\n",
    "    learning_rate = 0.01\n",
    "    hidden_size = 8\n",
    "    n_layers = 2\n",
    "    \n",
    "    state = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    max_step = 1000\n",
    "    \n",
    "    policy = PolicyNetwork(env.observation_space.shape[0], env.action_space.shape[0]).to(device)\n",
    "    \n",
    "    memory = ExperienceReplayBuffer(batch_size)\n",
    "\n",
    "    for _ in range(max_step):\n",
    "        # get a random action in this environment\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        next_state = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        gaus_param = policy(state)\n",
    "        \n",
    "        # print(gaus_param)\n",
    "        state = next_state\n",
    "        # render already plots the graph for you, no need to use plt\n",
    "        img = env.render()\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "\n",
    "# Sample an action from Normal distribution\n",
    "def choose_action(policy, state):  \n",
    "    \n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    act_prob, v = policy(state)\n",
    "    v = torch.exp(v)\n",
    "    pd = Normal(act_prob, torch.sqrt(v)) # get normal distirbution\n",
    "    action = pd.sample() # sample from it\n",
    "    log_prob = pd.log_prob(action)\n",
    "    return action, log_prob\n",
    "\n",
    "# task 3 * Make an episode reward processing function to turn one-step rewards into discounted rewards-to-go:\n",
    "#R(s_1) = sum_{t=1} gamma^{t-1} r_t, which is the discounted reward, starting from the state, s_1.   \n",
    "def dis_r_to_go(r_batch, gamma):\n",
    "    r_togo = []\n",
    "    R=0\n",
    "    for i in reversed(r_batch):\n",
    "        R = i + gamma*R\n",
    "        r_togo.insert(0, R)\n",
    "            \n",
    "    return r_togo\n",
    " \n",
    "    \n",
    "# task 4 Start the model by implementing a vanilla policy gradient agent, where the gradient ascent steps\n",
    "#are done with the average of the gradient of log-likelihood over a trajectory weight by rewards-to-go   \n",
    "#from each state. Try different step sizes in the gradient ascent.\n",
    "def run_trajectory(env_name, num_trajectory, gamma, learning_rate):\n",
    "    env = gym.make(env_name)\n",
    "    input_size=env.observation_space.shape[0]\n",
    "    output_size=env.action_space.shape[0]\n",
    "    hidden=32\n",
    "    policy = PolicyNetwork(input_size, output_size, hidden) #init policy function\n",
    "    optimizer = Adam(policy.parameters(), lr=learning_rate)\n",
    "     \n",
    "    obs_batch = [] # stores states\n",
    "    traj_r_batch = [] # all rewards in a trajectory\n",
    "    act_batch = [] # action batch\n",
    "    r_tg_batch=[] # reward to go batch\n",
    "    log_batch=[] # log prob batch\n",
    "    loss_batch=[] # loss batch\n",
    "    \n",
    "    count = 0\n",
    "    done=False\n",
    "\n",
    "    # number of trajectories\n",
    "    for eps in range(num_trajectory):\n",
    "        obs=env.reset() # restart\n",
    "        state=obs\n",
    "        \n",
    "        while (done==False and count<100): # time steps\n",
    "            action, log_prob = choose_action(policy, state)\n",
    "            act_batch.append(action)\n",
    "            log_batch.append(log_prob)\n",
    "            obs_batch.append(state)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            traj_r_batch.append(reward)\n",
    "            state=obs\n",
    "            count+=1\n",
    "            \n",
    "            \n",
    "                \n",
    "    r_tg_batch = dis_r_to_go(traj_r_batch, 0.95) # r to go\n",
    "    obs_batch = torch.tensor(obs_batch, dtype=torch.float)\n",
    "    #act_batch = torch.tensor(act_batch, dtype=torch.int64)\n",
    "    r_tg_batch = torch.tensor(r_tg_batch, dtype=torch.float)\n",
    "        \n",
    "        \n",
    "    return obs_batch, act_batch, r_tg_batch\n",
    "     \n",
    "def policy_gradient():\n",
    "    # gradient ascent by loss\n",
    "    lr=0.01\n",
    "    env_name='Pendulum-v1-custom'\n",
    "    num=10\n",
    "    gamma=0.95\n",
    "    obs_batch, act_batch, r_tg_batch = run_trajectory(env_name, num, gamma, lr)\n",
    "    loss = -(logp * weights).mean()\n",
    "    loss_batch.append(loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "    return \n",
    "\n",
    "'''Task 5: * Pendulum is a continuous action space environment. \n",
    "Check out the example in `Modules.py` for torch implementation of the Gaussian module.  \n",
    "(if you work in Julia, speak with me regarding the pendulum dynamics in Julia, and Flux for DNNs.)'''\n",
    "    \n",
    "\n",
    "'''Task 6: Add a feed-forward network for the critic, accepting the state, s=[sin(angle), cos(angle), angular velocity], \n",
    "and returning a scalar for the value of the state, s.'''\n",
    "def CriticNet(state):\n",
    "    input_size = state.shape[1]\n",
    "    output_size = 1\n",
    "    critic = CriticNetwork(input_size, output_size)\n",
    "    scalar = critic(torch.FloatTensor(state))\n",
    "    #scalar = CriticNetwork(input_size, output_size)\n",
    "    return scalar.item()\n",
    "    \n",
    "    \n",
    "'''Task 7 Implement the generalized advantage, see Eq11-12 in the PPO paper, to be used instead of rewards-to-go.'''\n",
    "\n",
    "def generalized_advantage(rewards, values, masks, gamma=0.99, lambd=0.95):\n",
    "    T = len(rewards)\n",
    "    gae = torch.zeros(1, 1)\n",
    "\n",
    "    advantages = torch.zeros(1, 1)\n",
    "    next_value = values[-1]\n",
    "\n",
    "    for t in reversed(range(T)):\n",
    "        delta = rewards[t] + gamma * next_value * masks[t] - values[t]\n",
    "        gae = gae * gamma * lambd * masks[t] + delta\n",
    "        advantages = torch.cat((gae, advantages))\n",
    "\n",
    "        next_value = values[t]\n",
    "\n",
    "    returns = values + advantages[:-1]\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    return returns, advantages\n",
    "\n",
    "     \n",
    "\n",
    "'''Task 8  Implement the surrogate objective for the policy gradient, see Eq7, with and without clipping. '''\n",
    "def surrogate_obj():\n",
    "    \n",
    "    \n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "'''Task 9 Implement the total loss, see Eq9 in the PPO. '''\n",
    "def PPO_eq9():\n",
    "    \n",
    "    \n",
    "    return \n",
    "\n",
    "\n",
    "'''Task 10 Combine all together to Algorithm 1 in the PPO paper. \n",
    "(In your basic implementation, you can collect data with a single actor, N=1)'''\n",
    "def algorithm_1():\n",
    "    \n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mybatch=[1,3,5,7,9]\n",
    "    #interaction_loop()\n",
    "    #test_experience_relay_buffer()\n",
    "    \n",
    "    \n",
    "    name = 'Pendulum-v1-custom'\n",
    "    #s_batch, a_batch, togo_batch = run_trajectory(name, 1, 0.95, 0.01)\n",
    "    \n",
    "    #criticNet(s_batch)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #res=dis_r_to_go(mybatch, 0.9)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce59663-0f12-42fe-bb75-62ca89e3fbaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfa6d39-ca21-4a00-86f6-a61ddbdbaecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9344b75b-f301-4f43-9587-8f0dc3473d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dazhouliu/opt/anaconda3/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'numpy.ndarray'>`\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v1-custom')\n",
    "#policy = PolicyNetwork(env.observation_space.shape[0], env.action_space.shape[0])\n",
    "\n",
    "obs=env.reset()\n",
    "obs = torch.from_numpy(obs).float().unsqueeze(0)\n",
    "#x1,x2 = policy(obs)\n",
    "#x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "866044c8-eef2-4293-9c60-3bb8ee17cbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "s=CriticNet(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84bf92fc-5895-42ce-a31c-563f5995645c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17903950810432434"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b6b4a3-11d8-422d-8d8f-c36af9b95110",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
