{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c56b6075-a47c-4016-a4af-12459e75814e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'hidden_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/q8/yg5ykqgx4xxbmzglr25_439r0000gn/T/ipykernel_33188/444194424.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;31m# test_general_advantage() # Task 7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;31m# test_surrogate_and_total_loss() # Task 8 & 9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m     \u001b[0mppo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Task 10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/q8/yg5ykqgx4xxbmzglr25_439r0000gn/T/ipykernel_33188/444194424.py\u001b[0m in \u001b[0;36mppo\u001b[0;34m()\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0minput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0moutput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m     \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPolicyNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m     \u001b[0mcritic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mValueNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0mactor_optimizer\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'hidden_size'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "from a3_gym_env.envs.pendulum import CustomPendulumEnv\n",
    "import numpy as np\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "from Modules import PolicyNetwork, ExperienceReplayBuffer, ValueNetwork\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.optim import Adam\n",
    "\n",
    "# import os\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "# [DONE] Task 1: Start by implementing an environment interaction loop. You may refer to homework 1 for inspiration.\n",
    "# [ ] Task 2: Create and test an experience replay buffer with a random policy, which is the Gaussian distribution with arbitrary (randomly initialized) weights of the policy feed-forward network,receiving state, s, and returning the mean, mu(s) and the log_std, log_stg(s) (natural logarithm of the standard deviation) of actions.  As mentioned above, you can use a state-independent standard variance.\n",
    "# [ ] Task 3: Make an episode reward processing function to turn one-step rewards into discounted rewards-to-go: R(s_1) = sum_{t=1} gamma^{t-1} r_t, which is the discounted reward, starting from the state, s_1.\n",
    "# [ ] Task 4: Start the model by implementing a vanilla policy gradient agent, where the gradient ascent stepsare done with the average of the gradient of log-likelihood over a trajectory weight by rewards-to-go   from each state. Try different step sizes in the gradient ascent.\n",
    "# [ ] Task 5: Pendulum is a continuous action space environment. Check out the example in `Modules.py` for torch implementation of the Gaussian module.  (if you work in Julia, speak with me regarding the pendulum dynamics in Julia, and Flux for DNNs.)\n",
    "# [ ] Task 6: Add a feed-forward network for the critic, accepting the state, s=[sin(angle), cos(angle), angular velocity], and returning a scalar for the value of the state, s.\n",
    "# [ ] Task 7: Implement the generalized advantage, see Eq11-12 in the PPO paper, to be used instead of rewards-to-go.\n",
    "# [ ] Task 8: Implement the surrogate objective for the policy gradient, see Eq7, without and without clipping.\n",
    "# [ ] Task 9: Implement the total loss, see Eq9 in the PPO.\n",
    "# [ ] Task 10: Combine all together to Algorithm 1 in the PPO paper. (In your basic implementation, you can collect data with a single actor, N=1)\n",
    "# [ ] Task 11: You should see progress with default hyperparameters, but you can try tuning those to see how it will improve your results.\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "def interaction_loop():\n",
    "    \"\"\"\"\n",
    "    * Start by implementing an environment interaction loop. You may refer to homework 1 for inspiration. \n",
    "    \"\"\"\n",
    "    env = gym.make(\"Pendulum-v1-custom\")\n",
    "    # sample hyperparameters\n",
    "    batch_size = 10000\n",
    "    epochs = 30\n",
    "    learning_rate = 1e-2\n",
    "    hidden_size = 8\n",
    "    n_layers = 2\n",
    "\n",
    "    # optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "\n",
    "    max_step = 1000\n",
    "    obs = env.reset()\n",
    "    for _ in range(max_step):\n",
    "        # get a random action in this environment\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        # render already plots the graph for you, no need to use plt\n",
    "        img = env.render()\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "    env.close()\n",
    "\n",
    "def get_action(mean, std=0.5):\n",
    "    # Create the covariance matrix\n",
    "    action_dim = 1\n",
    "    cov_var = torch.full(size=(action_dim,), fill_value=0.5)\n",
    "    cov_mat = torch.diag(cov_var)\n",
    "    dist = MultivariateNormal(mean.cpu(), cov_mat.cpu())\n",
    "\n",
    "    action = dist.sample()\n",
    "    log_prob = dist.log_prob(action)\n",
    "    return action.detach().numpy(), log_prob.detach()\n",
    "\n",
    "def test_experience_relay_buffer():\n",
    "    \"\"\"\n",
    "    * Create and test an experience replay buffer with a random policy, which is the \n",
    "    Gaussian distribution with arbitrary (randomly initialized) weights of the policy feed-forward network,\n",
    "    receiving state, s, and returning the mean, mu(s) and the log_std, log_stg(s) \n",
    "    (natural logarithm of the standard deviation) of actions.  As mentioned above, you can use \n",
    "    a state-independent standard variance.\n",
    "    \"\"\"\n",
    "    env = gym.make(\"Pendulum-v1-custom\")\n",
    "    # Hyperparameters\n",
    "    max_step = 200\n",
    "    batch_size = 10\n",
    "    max_buffer_size = 100\n",
    "    num_replay = 1\n",
    "\n",
    "    # Policy Network\n",
    "    input_size = env.observation_space.shape[0]\n",
    "    output_size = env.action_space.shape[0]\n",
    "    policy = PolicyNetwork(input_size, output_size).to(device)\n",
    "\n",
    "\n",
    "    # Replay Buffer\n",
    "    memory = ExperienceReplayBuffer(max_buffer_size)\n",
    "\n",
    "    # Env\n",
    "    state = env.reset()\n",
    "    state = torch.from_numpy(state).to(device)\n",
    "    for i in range(max_step):\n",
    "        # get a random action in this environment\n",
    "        # action = env.action_space.sample()\n",
    "        mean = policy(state)\n",
    "        action, _  = get_action(mean)\n",
    "\n",
    "        # Take a step\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        next_state = torch.from_numpy(obs).to(device)\n",
    "        reward = torch.tensor([reward], dtype=torch.float32, device=device)\n",
    "\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        if len(memory) > batch_size:\n",
    "            for _ in range(num_replay):\n",
    "                # sample from replay buffer\n",
    "                transitions = memory.sample(batch_size)\n",
    "                # uses the above transitions to optimize the policy\n",
    "                print(transitions)\n",
    "\n",
    "        state = next_state\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "    env.close()\n",
    "\n",
    "\n",
    "def compute_rtgs(batch_rews, gamma=0.95):\n",
    "  # The rewards-to-go (rtg) per episode per batch to return.\n",
    "  # The shape will be (num timesteps per episode)\n",
    "  batch_rtgs = []\n",
    "  # Iterate through each episode backwards to maintain same order\n",
    "  # in batch_rtgs\n",
    "  for ep_rews in reversed(batch_rews):\n",
    "    discounted_reward = 0 # The discounted reward so far\n",
    "    for rew in reversed(ep_rews):\n",
    "      discounted_reward = rew + discounted_reward * gamma\n",
    "      batch_rtgs.insert(0, discounted_reward)\n",
    "  # Convert the rewards-to-go into a tensor\n",
    "  batch_rtgs = torch.tensor(batch_rtgs, dtype=torch.float, requires_grad=True)\n",
    "  return batch_rtgs\n",
    "\n",
    "def evaluate(critic, batch_obs):\n",
    "    return critic(batch_obs).squeeze()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def rollout(env, policy, batch_size):\n",
    "    # Batch data\n",
    "    batch_obs = []             # batch observations\n",
    "    batch_acts = []            # batch actions\n",
    "    batch_log_probs = []       # log probs of each action\n",
    "    batch_rews = []            # batch rewards\n",
    "    batch_rtgs = []            # batch rewards-to-go\n",
    "    batch_lens = []            # episodic lengths in batch\n",
    "\n",
    "    t = 0\n",
    "    max_ep_t = batch_size\n",
    "    while t < batch_size:\n",
    "        ep_rews = []\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        for ep_t in range(max_ep_t):\n",
    "            t += 1\n",
    "            batch_obs.append(obs)\n",
    "            mean = policy(torch.from_numpy(obs).to(device))\n",
    "            action, log_prob  = get_action(mean)\n",
    "\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            ep_rews.append(reward)\n",
    "            batch_acts.append(action)\n",
    "            batch_log_probs.append(log_prob)\n",
    "            if done:\n",
    "                break\n",
    "        batch_lens.append(ep_t+1)\n",
    "        batch_rews.append(ep_rews)\n",
    "\n",
    "    # Reshape data as tensors in the shape specified before returning\n",
    "    batch_obs = torch.tensor(batch_obs, dtype=torch.float, requires_grad=True)\n",
    "    batch_acts = torch.tensor(batch_acts, dtype=torch.float, requires_grad=True)\n",
    "    batch_log_probs = torch.tensor(batch_log_probs, dtype=torch.float, requires_grad=True)\n",
    "    # ALG STEP #4\n",
    "    batch_rtgs = compute_rtgs(batch_rews)\n",
    "    # Return the batch data\n",
    "    return batch_obs, batch_acts, batch_log_probs, batch_rtgs, batch_lens \n",
    "\n",
    "def test_reward_to_go():\n",
    "    \"\"\"\n",
    "    * Make an episode reward processing function to turn one-step rewards into discounted rewards-to-go:\n",
    "    R(s_1) = sum_{t=1} gamma^{t-1} r_t, which is the discounted reward, starting from the state, s_1.\n",
    "    \"\"\"\n",
    "    env = gym.make(\"Pendulum-v1-custom\")\n",
    "    batch_size = 1000\n",
    "    # Policy Network\n",
    "    input_size = env.observation_space.shape[0]\n",
    "    output_size = env.action_space.shape[0]\n",
    "    policy = PolicyNetwork(input_size, output_size).to(device)\n",
    "\n",
    "    batch_obs, batch_acts, batch_log_probs, batch_rtgs, batch_lens = rollout(env, policy, batch_size)\n",
    "    print(batch_rtgs)\n",
    "\n",
    "\n",
    "def test_vanilla_gradient_ascent():\n",
    "    \"\"\"\n",
    "    * Start the model by implementing a vanilla policy gradient agent, where the gradient ascent steps\n",
    "    are done with the average of the gradient of log-likelihood over a trajectory weight by rewards-to-go   \n",
    "    from each state. Try different step sizes in the gradient ascent.  \n",
    "    \"\"\"\n",
    "    env = gym.make(\"Pendulum-v1-custom\")\n",
    "\n",
    "    epoch = 100\n",
    "    learning_rate = 0.95\n",
    "\n",
    "    batch_size = 1000\n",
    "    # Policy Network\n",
    "    input_size = env.observation_space.shape[0]\n",
    "    output_size = env.action_space.shape[0]\n",
    "    policy = PolicyNetwork(input_size, output_size).to(device)\n",
    "    optimizer =  Adam(policy.parameters(), lr=learning_rate)\n",
    "\n",
    "    for i in range(epoch):\n",
    "        batch_obs, batch_acts, batch_log_probs, batch_rtgs, batch_lens = rollout(env, policy, batch_size)\n",
    "        # average of log-likelihood, weighted by rewards-to-go\n",
    "\n",
    "        loss = -(batch_rtgs * batch_log_probs).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"{i}: {loss}\")\n",
    "\n",
    "def test_critic():\n",
    "    \"\"\"\n",
    "    * Add a feed-forward network for the critic, accepting the state, \n",
    "    s=[sin(angle), cos(angle), angular velocity], and returning a scalar for the value of the state, s.\n",
    "    \"\"\"\n",
    "    env = gym.make(\"Pendulum-v1-custom\")\n",
    "\n",
    "    epoch = 100\n",
    "    learning_rate = 0.95\n",
    "\n",
    "    batch_size = 1000\n",
    "    # Policy Network\n",
    "    input_size = env.observation_space.shape[0]\n",
    "    output_size = env.action_space.shape[0]\n",
    "    policy = PolicyNetwork(input_size, output_size).to(device)\n",
    "    critic = ValueNetwork(input_size, 1).to(device)\n",
    "    actor_optimizer =  Adam(policy.parameters(), lr=learning_rate)\n",
    "    critic_optimizer =  Adam(critic.parameters(), lr=learning_rate)\n",
    "\n",
    "    for i in range(epoch):\n",
    "        batch_obs, batch_acts, batch_log_probs, batch_rtgs, batch_lens = rollout(env, policy, batch_size)\n",
    "        # average of log-likelihood, weighted by rewards-to-go\n",
    "        actor_loss = -(batch_rtgs * batch_log_probs).mean()\n",
    "        V = critic(batch_obs.to(device)).squeeze().to(device)\n",
    "        critic_loss = torch.nn.MSELoss()(V, batch_rtgs.to(device))\n",
    "\n",
    "\n",
    "        actor_optimizer.zero_grad()\n",
    "        actor_loss.backward(retain_graph=True)\n",
    "        actor_optimizer.step()\n",
    "        \n",
    "        critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        critic_optimizer.step()\n",
    "        print(f\"{i}: {critic_loss}\")\n",
    "\n",
    "def test_general_advantage():\n",
    "    \"\"\"\n",
    "    * Implement the generalized advantage, see Eq11-12 in the PPO paper, to be used instead of rewards-to-go.\n",
    "    \"\"\"\n",
    "    env = gym.make(\"Pendulum-v1-custom\")\n",
    "\n",
    "    max_iteration = 100\n",
    "    epoch = 10\n",
    "    learning_rate = 0.95\n",
    "\n",
    "    batch_size = 1000\n",
    "    # Policy Network\n",
    "    input_size = env.observation_space.shape[0]\n",
    "    output_size = env.action_space.shape[0]\n",
    "    policy = PolicyNetwork(input_size, output_size).to(device)\n",
    "    critic = ValueNetwork(input_size, 1).to(device)\n",
    "    actor_optimizer =  Adam(policy.parameters(), lr=learning_rate)\n",
    "    critic_optimizer =  Adam(critic.parameters(), lr=learning_rate)\n",
    "\n",
    "    i = 0\n",
    "    while i < max_iteration:\n",
    "        batch_obs, batch_acts, batch_log_probs, batch_rtgs, batch_lens = rollout(env, policy, batch_size)\n",
    "        # average of log-likelihood, weighted by rewards-to-go\n",
    "        V = evaluate(critic, batch_obs.to(device)).to(device)\n",
    "        # General Advantage at iteration k \n",
    "        Ak = batch_rtgs.to(device) - V.detach()\n",
    "        print(f\"i={i}, Advantage: {Ak}\")\n",
    "        # BELOW is the optimization loop where we update our network for ei epoch\n",
    "        # for ei in range(epoch):\n",
    "        #     actor_loss = -(batch_rtgs * batch_log_probs).mean()\n",
    "        #     V = evaluate(batch_obs.to(device)).to(device)\n",
    "\n",
    "        #     critic_loss = torch.nn.MSELoss()(V, batch_rtgs.to(device))\n",
    "        #     actor_optimizer.zero_grad()\n",
    "        #     actor_loss.backward(retain_graph=True)\n",
    "        #     actor_optimizer.step()\n",
    "            \n",
    "        #     critic_optimizer.zero_grad()\n",
    "        #     critic_loss.backward()\n",
    "        #     critic_optimizer.step()\n",
    "        # print(f\"{i}: {critic_loss}\")\n",
    "        i += 1\n",
    "\n",
    "def test_surrogate_and_total_loss():\n",
    "    \"\"\"\n",
    "    * Implement the surrogate objective for the policy gradient, see Eq7, with and without clipping. \n",
    "    * Implement the total loss, see Eq9 in the PPO.    \n",
    "    \"\"\"\n",
    "    env = gym.make(\"Pendulum-v1-custom\")\n",
    "\n",
    "    max_iteration = 100\n",
    "    epoch = 10\n",
    "    learning_rate = 0.95\n",
    "\n",
    "\n",
    "    batch_size = 1000\n",
    "    # Policy Network\n",
    "    input_size = env.observation_space.shape[0]\n",
    "    output_size = env.action_space.shape[0]\n",
    "    policy = PolicyNetwork(input_size, output_size).to(device)\n",
    "    critic = ValueNetwork(input_size, 1).to(device)\n",
    "    actor_optimizer =  Adam(policy.parameters(), lr=learning_rate)\n",
    "    critic_optimizer =  Adam(critic.parameters(), lr=learning_rate)\n",
    "\n",
    "    cov_var = torch.full(size=(output_size,), fill_value=0.5)\n",
    "    cov_mat = torch.diag(cov_var)\n",
    "    clip = 0.2\n",
    "    i = 0\n",
    "    while i < max_iteration:\n",
    "        batch_obs, batch_acts, batch_log_probs, batch_rtgs, batch_lens = rollout(env, policy, batch_size)\n",
    "        # average of log-likelihood, weighted by rewards-to-go\n",
    "        Vk = evaluate(critic, batch_obs.to(device)).to(device)\n",
    "        # General Advantage at iteration k \n",
    "        Ak = batch_rtgs.to(device) - Vk.detach()\n",
    "        # BELOW is the optimization loop where we update our network for ei epoch\n",
    "        for ei in range(epoch):\n",
    "            actor_loss = -(batch_rtgs * batch_log_probs).mean()\n",
    "            V = evaluate(critic, batch_obs.to(device)).to(device)\n",
    "            # Calculate the log probability in this epoch using the updated actor\n",
    "            mean = policy(batch_obs.to(device))\n",
    "            dist = MultivariateNormal(mean.cpu(), cov_mat.cpu())\n",
    "            epoch_log_probs = dist.log_prob(batch_acts)\n",
    "\n",
    "            pi_ratios = torch.exp(epoch_log_probs - batch_log_probs).to(device)\n",
    "\n",
    "            # Surrogate Loss\n",
    "            surr = (pi_ratios * Ak).mean()\n",
    "            surr_clip = (torch.clamp(pi_ratios, 1 - clip, 1 + clip) * Ak).mean()\n",
    "            # Total Loss\n",
    "            total_loss = torch.min(surr, surr_clip)\n",
    "            print(f\"no_clip: {surr}; clip: {surr_clip}; total_loss: {total_loss}\" )\n",
    "\n",
    "\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward(retain_graph=True)\n",
    "            actor_optimizer.step()\n",
    "            \n",
    "        #     critic_loss = torch.nn.MSELoss()(V, batch_rtgs.to(device))\n",
    "        #     critic_optimizer.zero_grad()\n",
    "        #     critic_loss.backward()\n",
    "        #     critic_optimizer.step()\n",
    "        # print(f\"{i}: {critic_loss}\")\n",
    "        i += 1\n",
    "    \n",
    "def ppo():\n",
    "    \"\"\"\n",
    "    * Combine all together to Algorithm 1 in the PPO paper. (In your basic implementation, you can collect data with a single actor, N=1)\n",
    "    \"\"\"\n",
    "    env = gym.make(\"Pendulum-v1-custom\")\n",
    "\n",
    "    max_iteration = 100\n",
    "    epoch = 5 \n",
    "    learning_rate = 0.95\n",
    "\n",
    "\n",
    "    batch_size = 200\n",
    "    # Policy Network\n",
    "    input_size = env.observation_space.shape[0]\n",
    "    output_size = env.action_space.shape[0]\n",
    "    policy = PolicyNetwork(input_size, output_size).to(device)\n",
    "    critic = ValueNetwork(input_size, 1).to(device)\n",
    "    actor_optimizer =  Adam(policy.parameters(), lr=learning_rate)\n",
    "    critic_optimizer =  Adam(critic.parameters(), lr=learning_rate)\n",
    "\n",
    "    cov_var = torch.full(size=(output_size,), fill_value=0.5)\n",
    "    cov_mat = torch.diag(cov_var)\n",
    "    clip = 0.2\n",
    "    i = 0\n",
    "    while i < max_iteration:\n",
    "        batch_obs, batch_acts, batch_log_probs, batch_rtgs, batch_lens = rollout(env, policy, batch_size)\n",
    "        # average of log-likelihood, weighted by rewards-to-go\n",
    "        Vk = evaluate(critic, batch_obs.to(device)).to(device)\n",
    "        # General Advantage at iteration k \n",
    "        Ak = batch_rtgs.to(device) - Vk.detach()\n",
    "        # BELOW is the optimization loop where we update our network for ei epoch\n",
    "        for ei in range(epoch):\n",
    "            V = evaluate(critic, batch_obs.to(device)).to(device)\n",
    "            # Calculate the log probability in this epoch using the updated actor\n",
    "            mean = policy(batch_obs.to(device))\n",
    "            dist = MultivariateNormal(mean.cpu(), cov_mat.cpu())\n",
    "            epoch_log_probs = dist.log_prob(batch_acts)\n",
    "\n",
    "            pi_ratios = torch.exp(epoch_log_probs - batch_log_probs).to(device)\n",
    "\n",
    "            # Surrogate Loss\n",
    "            surr = (pi_ratios * Ak).mean()\n",
    "            surr_clip = (torch.clamp(pi_ratios, 1 - clip, 1 + clip) * Ak).mean()\n",
    "\n",
    "            actor_loss = -torch.min(surr, surr_clip)\n",
    "            critic_loss = torch.nn.MSELoss()(V, batch_rtgs.to(device))\n",
    "\n",
    "            print(f\"actor_loss: {actor_loss}, citic_loss: {critic_loss}\")\n",
    "\n",
    "\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward(retain_graph=True)\n",
    "            actor_optimizer.step()\n",
    "            \n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "        # print(f\"{i}: {critic_loss}\")\n",
    "        i += 1\n",
    "    torch.save(policy.state_dict(), './ppo_actor.pth')\n",
    "    torch.save(critic.state_dict(), './ppo_critic.pth')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # interaction_loop() # Task 1\n",
    "    # test_experience_relay_buffer() # Task 2\n",
    "    # test_reward_to_go() # Task 3\n",
    "    # test_vanilla_gradient_ascent() # Task 4\n",
    "    # Checked Out Module # Task 5\n",
    "    # test_critic() # Task 6\n",
    "    # test_general_advantage() # Task 7\n",
    "    # test_surrogate_and_total_loss() # Task 8 & 9\n",
    "    ppo() # Task 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb47e37-a959-4b2d-a313-cefe993b8cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
