{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f0e4871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-137.69229125976562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q8/yg5ykqgx4xxbmzglr25_439r0000gn/T/ipykernel_80596/1981483838.py:159: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  traj_r = np.array(traj_r)\n",
      "/var/folders/q8/yg5ykqgx4xxbmzglr25_439r0000gn/T/ipykernel_80596/1981483838.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  traj_r = np.array(traj_r)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "from a3_gym_env.envs.pendulum import CustomPendulumEnv\n",
    "import numpy as np\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "from torch.distributions import Normal\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from Modules import PolicyNetwork, ExperienceReplayBuffer, CriticNetwork\n",
    "\n",
    "# [DONE] Task 1: Start by implementing an environment interaction loop. You may refer to homework 1 for inspiration.\n",
    "# [ ] Task 2: Create and test an experience replay buffer with a random policy, which is the Gaussian distribution with arbitrary (randomly initialized) weights of the policy feed-forward network,receiving state, s, and returning the mean, mu(s) and the log_std, log_stg(s) (natural logarithm of the standard deviation) of actions.  As mentioned above, you can use a state-independent standard variance.\n",
    "# [ ] Task 3: Make an episode reward processing function to turn one-step rewards into discounted rewards-to-go: R(s_1) = sum_{t=1} gamma^{t-1} r_t, which is the discounted reward, starting from the state, s_1.\n",
    "# [ ] Task 4: Start the model by implementing a vanilla policy gradient agent, where the gradient ascent stepsare done with the average of the gradient of log-likelihood over a trajectory weight by rewards-to-go   from each state. Try different step sizes in the gradient ascent.\n",
    "# [ ] Task 5: Pendulum is a continuous action space environment. Check out the example in `Modules.py` for torch implementation of the Gaussian module.  (if you work in Julia, speak with me regarding the pendulum dynamics in Julia, and Flux for DNNs.)\n",
    "# [ ] Task 6: Add a feed-forward network for the critic, accepting the state, s=[sin(angle), cos(angle), angular velocity], and returning a scalar for the value of the state, s.\n",
    "# [ ] Task 7: Implement the generalized advantage, see Eq11-12 in the PPO paper, to be used instead of rewards-to-go.\n",
    "# [ ] Task 8: Implement the surrogate objective for the policy gradient, see Eq7, without and without clipping.\n",
    "# [ ] Task 9: Implement the total loss, see Eq9 in the PPO.\n",
    "# [ ] Task 10: Combine all together to Algorithm 1 in the PPO paper. (In your basic implementation, you can collect data with a single actor, N=1)\n",
    "# [ ] Task 11: You should see progress with default hyperparameters, but you can try tuning those to see how it will improve your results.\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "def interaction_loop():\n",
    "    env = gym.make(\"Pendulum-v1-custom\")\n",
    "    # sample hyperparameters\n",
    "    batch_size = 1000\n",
    "    epochs = 30\n",
    "    learning_rate = 1e-2\n",
    "    #hidden_size = 8\n",
    "    #n_layers = 2\n",
    "\n",
    "    # optimizer = optim.Adam(vae.parameters(), lr=learning_rate)\n",
    "\n",
    "    max_step = 1000\n",
    "    obs = env.reset()\n",
    "    for _ in range(max_step):\n",
    "        # get a random action in this environment\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        # render already plots the graph for you, no need to use plt\n",
    "        img = env.render()\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "    env.close()\n",
    "\n",
    "    \n",
    "# task 2 * Create and test an experience replay buffer with a random policy, which is the \n",
    "#Gaussian distribution with arbitrary (randomly initialized) weights of the policy feed-forward network,\n",
    "#receiving state, s, and returning the mean, mu(s) and the log_std, log_stg(s) \n",
    "#(natural logarithm of the standard deviation) of actions.  As mentioned above, you can use \n",
    "#a state-independent standard variance.\n",
    "def test_experience_relay_buffer():\n",
    "    env = gym.make(\"Pendulum-v1-custom\")\n",
    "    # sample hyperparameters\n",
    "    batch_size = 1000\n",
    "    epochs = 30\n",
    "    learning_rate = 0.01\n",
    "    hidden_size = 8\n",
    "    n_layers = 2\n",
    "    \n",
    "    state = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    max_step = 1000\n",
    "    \n",
    "    policy = PolicyNetwork(env.observation_space.shape[0], env.action_space.shape[0]).to(device)\n",
    "    \n",
    "    memory = ExperienceReplayBuffer(batch_size)\n",
    "\n",
    "    for _ in range(max_step):\n",
    "        # get a random action in this environment\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        next_state = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        gaus_param = policy(state)\n",
    "        \n",
    "        # print(gaus_param)\n",
    "        state = next_state\n",
    "        # render already plots the graph for you, no need to use plt\n",
    "        img = env.render()\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "\n",
    "# Sample an action from Normal distribution\n",
    "def choose_action(policy, state):  \n",
    "    \n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    act_prob, v = policy(state)\n",
    "    v = torch.exp(v)\n",
    "    pd = Normal(act_prob, torch.sqrt(v)) # get normal distirbution\n",
    "    action = pd.sample() # sample from it\n",
    "    log_prob = pd.log_prob(action)\n",
    "    return action, log_prob\n",
    "\n",
    "# task 3 * Make an episode reward processing function to turn one-step rewards into discounted rewards-to-go:\n",
    "#R(s_1) = sum_{t=1} gamma^{t-1} r_t, which is the discounted reward, starting from the state, s_1.   \n",
    "def dis_r_to_go(r_batch, gamma):\n",
    "    r_togo = np.zeros(len(r_batch))\n",
    "    R=0\n",
    "    for i in reversed(range(len(r_batch))):\n",
    "        R = r_batch[i] + gamma*R\n",
    "        r_togo[i] = R\n",
    "            \n",
    "    return r_togo\n",
    " \n",
    "    \n",
    "# task 4 Start the model by implementing a vanilla policy gradient agent, where the gradient ascent steps\n",
    "#are done with the average of the gradient of log-likelihood over a trajectory weight by rewards-to-go   \n",
    "#from each state. Try different step sizes in the gradient ascent.\n",
    "def run_trajectory(env_name, num_trajectory, gamma, learning_rate):\n",
    "    env = gym.make(env_name)\n",
    "    batch_size=500\n",
    "    input_size=env.observation_space.shape[0]\n",
    "    output_size=env.action_space.shape[0]\n",
    "    hidden=32\n",
    "    policy = PolicyNetwork(input_size, output_size, hidden) #init policy function\n",
    "    optimizer = Adam(policy.parameters(), lr=learning_rate)\n",
    "     \n",
    "    obs_batch = [] # stores states\n",
    "    traj_r_batch = [] # all rewards in a trajectory\n",
    "    act_batch = [] # action batch\n",
    "    r_tg_batch=[] # reward to go batch\n",
    "    log_batch=[] # log prob batch\n",
    "    loss_batch=[] # loss batch\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "\n",
    "    # number of trajectories\n",
    "    for eps in range(num_trajectory):\n",
    "        obs=env.reset() # restart\n",
    "        done=False\n",
    "        traj_r = []\n",
    "        #state=obs\n",
    "        \n",
    "        while (count<batch_size): # time steps\n",
    "            action, log_prob = choose_action(policy, obs)\n",
    "            act_batch.append(action)\n",
    "            log_batch.append(log_prob)\n",
    "            obs_batch.append(obs)\n",
    "            \n",
    "            obs, reward, done, info = env.step(action)\n",
    "            traj_r.append(reward)\n",
    "            count+=1\n",
    "            \n",
    "        traj_r = np.array(traj_r)\n",
    "        traj_r_tg = np.zeros_like(traj_r)\n",
    "        traj_r_tg = dis_r_to_go(traj_r, 0.95) # r to go\n",
    "                \n",
    "        r_tg_batch.append(traj_r_tg) \n",
    "        \n",
    "        \n",
    "    obs_batch = torch.tensor(obs_batch, dtype=torch.float, requires_grad=True)\n",
    "    act_batch = torch.tensor(act_batch, dtype=torch.float,requires_grad=True)\n",
    "    r_tg_batch = torch.tensor(r_tg_batch, dtype=torch.float,requires_grad=True)\n",
    "    log_batch = torch.tensor(log_batch, dtype=torch.float,requires_grad=True)\n",
    "    \n",
    "    \n",
    "    loss = -(log_batch * r_tg_batch).mean() # gradient ascent\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "   \n",
    "\n",
    "    print(loss.item())   \n",
    "    return obs_batch, act_batch, r_tg_batch, log_batch, loss.item()\n",
    "     \n",
    "\n",
    "'''Task 5: * Pendulum is a continuous action space environment. \n",
    "Check out the example in `Modules.py` for torch implementation of the Gaussian module.  \n",
    "(if you work in Julia, speak with me regarding the pendulum dynamics in Julia, and Flux for DNNs.)'''\n",
    "    \n",
    "\n",
    "'''Task 6: Add a feed-forward network for the critic, accepting the state, s=[sin(angle), cos(angle), angular velocity], \n",
    "and returning a scalar for the value of the state, s.'''\n",
    "def CriticNet(state):\n",
    "    #input_size = state.shape\n",
    "    input_size=3\n",
    "    output_size = 1\n",
    "    critic = CriticNetwork(input_size, output_size)\n",
    "    scalar = critic(torch.FloatTensor(state))\n",
    "    #scalar = critic(state)\n",
    "    return scalar.item()\n",
    "\n",
    "# calculate values of state of actor network\n",
    "def calc_value(state_batch):\n",
    "    l = state_batch.size(dim=0)\n",
    "    value_batch = np.zeros(l)\n",
    "    my_scalar = 0\n",
    "    for i in range(l):\n",
    "        my_scalar = CriticNet(state_batch[i])\n",
    "        value_batch[i] = my_scalar\n",
    "        #print(value_batch[i])\n",
    "    value_batch = torch.tensor(value_batch, dtype=torch.float)\n",
    "    return value_batch\n",
    "    \n",
    "    \n",
    "'''Task 7 Implement the generalized advantage, see Eq11-12 in the PPO paper, to be used instead of rewards-to-go.'''\n",
    "# advantage estimator\n",
    "def generalized_advantage(r_batch, value, gamma=0.99, lamd=0.95):\n",
    "    gae = 0\n",
    "    b_size = r_batch.size(dim=1)\n",
    "    adv = np.zeros(b_size) # batch for generalized advantage\n",
    "    next_value = 0\n",
    "\n",
    "    for t in reversed(range(b_size)): # \n",
    "        b_error = r_batch[0][t] + gamma * next_value - value[t]\n",
    "        gae = b_error + gae * gamma * lamd\n",
    "        next_value = value[t]\n",
    "        adv[t] = gae\n",
    "        \n",
    "        #print(gae)\n",
    "    adv = torch.tensor(adv, dtype=torch.float)\n",
    "\n",
    "    return adv\n",
    "\n",
    "\n",
    "\n",
    "'''Task 8  Implement the surrogate objective for the policy gradient, see Eq7, with and without clipping. '''\n",
    "def surrogate_obj(log_batch, A_head, epsilon):\n",
    "    \n",
    "    cpi = np.zeros(log_batch.size(dim=0))\n",
    "    sur_obj = np.zeros(log_batch.size(dim=0))\n",
    "    for i in reversed(range(log_batch.size(dim=0))):\n",
    "        if (i>0):\n",
    "            cur_policy = log_batch[i]\n",
    "            ratio = cur_policy/log_batch[i-1]\n",
    "            cpi[i] = ratio * A_head[i] # conservative policy iteration\n",
    "            clip_ratio = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n",
    "            sur_obj[i] = torch.min(cpi[i], (clip_ratio*A_head[i]))\n",
    "            #print(type(cpi[i]))\n",
    "            #print(type(cpi))\n",
    "    \n",
    "    cpi = torch.tensor(cpi, dtype=torch.float)\n",
    "    sur_obj = torch.tensor(sur_obj, dtype=torch.float)\n",
    "    return sur_obj, cpi\n",
    "\n",
    "\n",
    "'''Task 9 Implement the total loss, see Eq9 in the PPO. '''\n",
    "def PPO_eq9():\n",
    "    \n",
    "     \n",
    "    \n",
    "    return 0\n",
    "\n",
    "\n",
    "'''Task 10 Combine all together to Algorithm 1 in the PPO paper. \n",
    "(In your basic implementation, you can collect data with a single actor, N=1)'''\n",
    "def algorithm_1():\n",
    "    #for iteration in range():\n",
    "     #   for actor in range():\n",
    "            \n",
    "    \n",
    "    return 0\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mybatch=[1,3,5,7,9]\n",
    "    #interaction_loop()\n",
    "    #test_experience_relay_buffer()\n",
    "    \n",
    "    \n",
    "    name = 'Pendulum-v1-custom'\n",
    "    \n",
    "    s_batch, a_batch, togo_batch, log_batch, loss = run_trajectory(name, 1, 0.95, 0.01)\n",
    "    \n",
    "    #criticNet(s_batch)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #res=dis_r_to_go(mybatch, 0.9)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f59f8e1-7406-4f6c-801a-ab867ba5cebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7289, -0.6846,  0.3521],\n",
       "        [ 0.7197, -0.6942, -0.2669],\n",
       "        [ 0.7021, -0.7121, -0.5023],\n",
       "        ...,\n",
       "        [ 0.2781,  0.9606,  6.2948],\n",
       "        [-0.0754,  0.9972,  7.1449],\n",
       "        [-0.4577,  0.8891,  8.0000]], requires_grad=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "866044c8-eef2-4293-9c60-3bb8ee17cbfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-137.69229125976562"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b5c0da5-a64c-4db4-ac90-4e0f9b36c401",
   "metadata": {},
   "outputs": [],
   "source": [
    "value=calc_value(s_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f13f0c45-1e40-4d33-a1e1-16a2c5feaac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b4de462-aa72-4651-8c19-224b2a9e0547",
   "metadata": {},
   "outputs": [],
   "source": [
    "#value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55b6b4a3-11d8-422d-8d8f-c36af9b95110",
   "metadata": {},
   "outputs": [],
   "source": [
    "c=generalized_advantage(togo_batch, value, gamma=0.99, lamd=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c13a1b20-16ee-4a44-ae39-0b2b90201684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1587.7316, -1595.4407, -1598.9510, -1598.4623, -1593.3239, -1583.2383,\n",
       "        -1568.0068, -1547.2717, -1521.4280, -1491.1184, -1457.3116, -1420.6160,\n",
       "        -1383.7124, -1348.9764, -1319.8510, -1298.3645, -1284.8042, -1277.1028,\n",
       "        -1273.0343, -1272.2870, -1272.3955, -1272.9189, -1271.9141, -1268.9266,\n",
       "        -1263.2604, -1253.9785, -1240.8010, -1223.3254, -1201.3940, -1175.1576,\n",
       "        -1144.8004, -1111.2556, -1075.9952, -1041.3121, -1010.0479,  -986.0060,\n",
       "         -971.9357,  -967.9603,  -972.6788,  -984.5463, -1002.2635, -1024.4709,\n",
       "        -1050.3668, -1079.0642, -1109.5378, -1141.7629, -1174.9819, -1208.8773,\n",
       "        -1243.8253, -1279.1874, -1314.6576, -1350.5012, -1386.3195, -1421.8701,\n",
       "        -1457.3545, -1492.1636, -1525.9115, -1558.9495, -1590.5607, -1620.8546,\n",
       "        -1649.3738, -1676.0085, -1700.2755, -1721.5626, -1739.9891, -1755.0481,\n",
       "        -1766.3724, -1773.6063, -1776.4425, -1774.4092, -1767.5999, -1755.3102,\n",
       "        -1738.5145, -1718.0291, -1693.6169, -1667.5770, -1641.6086, -1617.7405,\n",
       "        -1599.5818, -1590.0143, -1588.0431, -1593.6636, -1603.7798, -1618.0094,\n",
       "        -1634.3472, -1651.6536, -1668.9900, -1684.5537, -1698.4014, -1709.5992,\n",
       "        -1717.6356, -1722.2809, -1722.6783, -1719.0801, -1711.0334, -1699.1052,\n",
       "        -1683.5781, -1665.2524, -1644.9335, -1624.9237, -1606.3306, -1592.6697,\n",
       "        -1586.6545, -1588.8535, -1598.8322, -1614.4779, -1635.2714, -1659.2091,\n",
       "        -1685.4418, -1713.0861, -1740.9232, -1768.4976, -1794.8680, -1819.9523,\n",
       "        -1842.9214, -1863.4010, -1880.6837, -1894.2413, -1904.2147, -1909.2593,\n",
       "        -1909.6526, -1905.4182, -1896.1465, -1882.1055, -1863.9575, -1842.3711,\n",
       "        -1818.8435, -1795.0918, -1773.6316, -1757.5558, -1748.6230, -1747.1160,\n",
       "        -1751.2781, -1758.8003, -1769.1279, -1780.6902, -1791.5050, -1801.2454,\n",
       "        -1808.7543, -1812.9058, -1813.3112, -1809.4951, -1801.2715, -1788.4980,\n",
       "        -1771.8221, -1751.1458, -1728.3925, -1704.2084, -1681.7926, -1663.8567,\n",
       "        -1653.3575, -1650.9542, -1653.9186, -1662.3706, -1674.1893, -1687.9360,\n",
       "        -1702.3143, -1716.6816, -1729.0116, -1739.3676, -1746.5757, -1749.9467,\n",
       "        -1749.1146, -1743.1680, -1732.1238, -1715.9500, -1695.3430, -1669.8043,\n",
       "        -1641.7191, -1612.0374, -1582.8148, -1555.1884, -1533.2272, -1519.0258,\n",
       "        -1511.6996, -1509.4552, -1511.3018, -1515.6099, -1519.6918, -1522.9569,\n",
       "        -1524.8938, -1524.3634, -1520.6921, -1513.4604, -1502.4611, -1487.8838,\n",
       "        -1469.4202, -1448.0693, -1424.5533, -1399.7322, -1375.1617, -1354.3524,\n",
       "        -1338.4928, -1329.8423, -1330.1482, -1338.4163, -1351.6274, -1370.3453,\n",
       "        -1392.9712, -1418.6068, -1445.9005, -1474.4406, -1503.5061, -1532.5742,\n",
       "        -1561.1195, -1588.6708, -1614.9901, -1639.1062, -1661.1729, -1680.4774,\n",
       "        -1696.7554, -1709.7848, -1719.0613, -1723.7800, -1723.9163, -1719.2128,\n",
       "        -1709.8243, -1696.0344, -1676.3008, -1653.2268, -1627.8866, -1600.8760,\n",
       "        -1575.8848, -1555.2910, -1543.4073, -1539.6372, -1544.9186, -1556.5129,\n",
       "        -1573.4009, -1593.9900, -1616.7107, -1640.1869, -1663.3872, -1685.7535,\n",
       "        -1706.4966, -1725.0000, -1740.6919, -1753.2219, -1761.7894, -1766.3804,\n",
       "        -1766.3574, -1761.7183, -1752.4562, -1738.4104, -1720.6948, -1700.3530,\n",
       "        -1678.1487, -1657.0641, -1639.8098, -1630.2833, -1631.9701, -1641.0582,\n",
       "        -1659.2483, -1681.9657, -1709.9948, -1740.7886, -1773.5486, -1807.4955,\n",
       "        -1841.7646, -1875.4050, -1908.2629, -1939.1279, -1968.2847, -1994.6560,\n",
       "        -2017.9558, -2038.0065, -2053.5325, -2065.0288, -2072.1362, -2073.7883,\n",
       "        -2070.6067, -2062.1965, -2049.0940, -2031.7802, -2011.7281, -1989.9666,\n",
       "        -1970.9762, -1956.7007, -1952.0758, -1956.6653, -1968.9124, -1988.3953,\n",
       "        -2010.9559, -2037.0461, -2064.4065, -2091.9431, -2117.5769, -2141.4502,\n",
       "        -2162.9368, -2180.3428, -2193.8835, -2202.7827, -2206.6650, -2204.7632,\n",
       "        -2197.1655, -2183.9663, -2165.3711, -2142.8887, -2116.8472, -2090.8994,\n",
       "        -2067.9111, -2050.4619, -2042.5248, -2041.9143, -2048.1592, -2059.1213,\n",
       "        -2073.4556, -2088.4248, -2103.7610, -2117.6287, -2128.9036, -2136.7502,\n",
       "        -2140.1069, -2138.3662, -2131.0725, -2118.1262, -2099.8523, -2076.6033,\n",
       "        -2049.6855, -2021.1763, -1993.0425, -1968.8417, -1951.1582, -1942.7123,\n",
       "        -1941.5662, -1946.1973, -1954.4042, -1965.6487, -1977.0283, -1988.5801,\n",
       "        -1997.7302, -2003.8406, -2006.1783, -2004.0773, -1996.6661, -1983.8108,\n",
       "        -1965.2988, -1941.1478, -1912.3796, -1880.1733, -1847.2494, -1814.4795,\n",
       "        -1786.5149, -1765.5161, -1753.6816, -1749.3616, -1751.1227, -1757.5477,\n",
       "        -1766.6418, -1777.1680, -1786.2300, -1794.1896, -1799.9061, -1802.0199,\n",
       "        -1800.4465, -1794.5496, -1783.1975, -1767.6975, -1748.0156, -1724.3182,\n",
       "        -1698.1539, -1671.7822, -1647.8726, -1629.6279, -1620.6053, -1621.7194,\n",
       "        -1632.4642, -1650.8374, -1675.3923, -1704.5566, -1736.3542, -1769.5979,\n",
       "        -1803.2303, -1836.6169, -1869.5321, -1901.2776, -1931.4532, -1959.4620,\n",
       "        -1984.5020, -2006.5122, -2024.8702, -2038.8480, -2048.3660, -2052.9463,\n",
       "        -2052.1782, -2045.9298, -2034.5636, -2017.0443, -1995.6569, -1971.1226,\n",
       "        -1945.9856, -1922.0217, -1903.9823, -1894.7549, -1894.6548, -1902.2489,\n",
       "        -1914.8127, -1932.3881, -1950.1870, -1969.7463, -1988.8218, -2006.3888,\n",
       "        -2021.4679, -2033.2235, -2041.0909, -2044.1425, -2042.0582, -2034.6941,\n",
       "        -2021.5831, -2002.8932, -1978.9857, -1950.8374, -1920.0481, -1890.0679,\n",
       "        -1861.6947, -1840.2869, -1828.5818, -1824.5502, -1827.2164, -1835.2677,\n",
       "        -1844.7434, -1856.1534, -1867.1749, -1877.1273, -1884.1172, -1888.0406,\n",
       "        -1887.4625, -1882.3550, -1871.5228, -1856.1256, -1834.0776, -1806.9359,\n",
       "        -1775.6740, -1741.5643, -1706.2307, -1672.5084, -1643.9248, -1623.3453,\n",
       "        -1611.6613, -1607.1530, -1607.7085, -1612.2574, -1618.1960, -1624.9141,\n",
       "        -1631.2328, -1635.8655, -1636.7026, -1634.7833, -1628.4537, -1617.2465,\n",
       "        -1600.6515, -1578.3420, -1549.9069, -1515.5530, -1475.0173, -1430.3879,\n",
       "        -1381.3953, -1331.4806, -1283.6033, -1240.7365, -1205.9012, -1178.7189,\n",
       "        -1158.6902, -1143.6320, -1131.3588, -1120.7717, -1110.3220, -1098.2542,\n",
       "        -1083.8092, -1066.0739, -1043.9312, -1017.2365,  -985.2745,  -947.7221,\n",
       "         -904.1254,  -854.4613,  -799.1112,  -737.8417,  -672.8728,  -605.1343,\n",
       "         -538.1810,  -474.5871,  -417.9382,  -371.0072,  -332.5252,  -301.0126,\n",
       "         -275.2288,  -253.3296,  -232.5355,  -213.7643,  -195.3801,  -177.2009,\n",
       "         -158.0321,  -137.9850,  -116.4040,   -94.4370,   -71.3486,   -49.2458,\n",
       "          -27.9004,   -10.5872])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fbb210e8-8663-495c-9e31-1de6102e2e1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "min() received an invalid combination of arguments - got (numpy.float64, Tensor), but expected one of:\n * (Tensor input)\n * (Tensor input, Tensor other, *, Tensor out)\n * (Tensor input, int dim, bool keepdim, *, tuple of Tensors out)\n * (Tensor input, name dim, bool keepdim, *, tuple of Tensors out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/q8/yg5ykqgx4xxbmzglr25_439r0000gn/T/ipykernel_80596/739606997.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msurrogate_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/q8/yg5ykqgx4xxbmzglr25_439r0000gn/T/ipykernel_80596/1981483838.py\u001b[0m in \u001b[0;36msurrogate_obj\u001b[0;34m(log_batch, A_head, epsilon)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0mcpi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mratio\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mA_head\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# conservative policy iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0mclip_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0msur_obj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mclip_ratio\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mA_head\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m             \u001b[0;31m#print(type(cpi[i]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0;31m#print(type(cpi))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: min() received an invalid combination of arguments - got (numpy.float64, Tensor), but expected one of:\n * (Tensor input)\n * (Tensor input, Tensor other, *, Tensor out)\n * (Tensor input, int dim, bool keepdim, *, tuple of Tensors out)\n * (Tensor input, name dim, bool keepdim, *, tuple of Tensors out)\n"
     ]
    }
   ],
   "source": [
    "surrogate_obj(log_batch, c, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a44d66-fa7a-441a-a162-db566deeef37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
